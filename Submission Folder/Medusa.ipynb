{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d6b6a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.firefox_binary import FirefoxBinary\n",
    "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd389cc",
   "metadata": {},
   "source": [
    "## 1) Tor-Selenium Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84067d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "torexe = os.popen(r'C:\\Users\\Daniel\\Desktop\\Tor Browser\\Browser\\TorBrowser\\Tor\\tor.exe')\n",
    "\n",
    "firefoxBinary = r\"C:\\Users\\Daniel\\Desktop\\Tor Browser\\Browser\\firefox.exe\"\n",
    "geckodriverPath = r\"C:\\Users\\Daniel\\Desktop\\Tor Browser\\geckodriver.exe\"\n",
    "proxyIP = \"127.0.0.1\"\n",
    "proxyPort = 9150\n",
    "\n",
    "binary = FirefoxBinary(firefoxBinary)\n",
    "\n",
    "proxy_settings = {\n",
    "    \"network.proxy.type\": 1,\n",
    "    \"network.proxy.socks\": proxyIP,\n",
    "    \"network.proxy.socks_port\": proxyPort,\n",
    "    \"network.proxy.socks_remote_dns\": False,\n",
    "    \"extensions.torlauncher.start_tor\": True\n",
    "}\n",
    "\n",
    "profile = FirefoxProfile(r'C:\\Users\\Daniel\\Desktop\\Tor Browser\\Browser\\TorBrowser\\Data\\Browser\\profile.default')\n",
    "profile.set_preference('extensions.torlauncher.start_tor', True) # bypass tor connection page\n",
    "profile.set_preference('network.proxy.type', 1)\n",
    "profile.set_preference('network.proxy.socks', '127.0.0.1')\n",
    "profile.set_preference('network.proxy.socks_port', 9150)\n",
    "profile.set_preference(\"network.proxy.socks_remote_dns\", True)\n",
    "profile.update_preferences()\n",
    "\n",
    "driver = webdriver.Firefox(executable_path=geckodriverPath, \n",
    "                           firefox_binary=binary, \n",
    "                           firefox_profile=profile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9c2b8f",
   "metadata": {},
   "source": [
    "## 2) Scraping Medusa\n",
    "\n",
    "Obtain all information about threatened companies posted in Medusa Blog  \n",
    "- Company Name  \n",
    "- Company Description  \n",
    "- Date Published by Medusa  \n",
    "- Number of clicks  \n",
    "- Type of data stolen (not sure about how to extract this - i think we need to do this manually)\n",
    "\n",
    "BlackMatter link: http://medusaxko7jxtrojdkxo66j7ck4q5tgktf7uqsqyfry4ebnxlcbkccyd.onion/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbfe371",
   "metadata": {},
   "source": [
    "Medusa Blog is displayed in a timeline format where content is continually loaded as you scroll down to the end of the webpage. This means we need to scroll all the way down, then extract the information.  \n",
    "\n",
    "To know when the webpage is fully loaded, either scroll all the way down each iteration until there is no new content loaded after an explicit wait, or search for the absence of HTML element \"progress\" after scrolling all the way down each iteration.  \n",
    "\n",
    "Each company information is in the form of the HTML element \"Card\" and have the class \"card\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3b13510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait till your tor browser is connected to the tor network before executing this\n",
    "driver.get('http://medusaxko7jxtrojdkxo66j7ck4q5tgktf7uqsqyfry4ebnxlcbkccyd.onion/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd10329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCROLL_PAUSE_TIME = 5 # as the Tor browser is slow, might need to increase this accordingly\n",
    "\n",
    "# Get scroll height\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    # Scroll down to bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Wait to load page\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4548bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "companyNames = []\n",
    "companyDescription = []\n",
    "dateVictimised = []\n",
    "noClicks = []\n",
    "\n",
    "cards = driver.find_elements_by_xpath(\"//div[@class='card']\")\n",
    "\n",
    "for card in cards:\n",
    "    dateVictimisedElement = card.find_element_by_xpath(\".//div[@class='date-updated']/span[@class='text-muted']\")\n",
    "    dateVictimisedText = dateVictimisedElement.text\n",
    "    # if dateVictimisedText[0:4] != \"2023\":\n",
    "    #     continue\n",
    "    dateVictimised.append(dateVictimisedText)\n",
    "\n",
    "    companyNameElement = card.find_element_by_xpath(\".//h3[@class='card-title']\")\n",
    "    companyNameText = companyNameElement.text\n",
    "    companyNames.append(companyNameText)\n",
    "\n",
    "    companyDescriptionElement = card.find_element_by_xpath(\".//p[@class='card-text text-left']\")\n",
    "    companyDescriptionText = companyDescriptionElement.text\n",
    "    companyDescription.append(companyDescriptionText)\n",
    "\n",
    "    noClicksElement = card.find_element_by_xpath(\".//div[@class='number-view']/span[@class='text-muted']\")\n",
    "    noClicksText = noClicksElement.text\n",
    "    noClicks.append(noClicksText)\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235c02a9",
   "metadata": {},
   "source": [
    "## 3) Clean data + Extract to CSV/Excel format\n",
    "\n",
    "\n",
    "Requirements for data:  \n",
    "- Date of victim company within 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492efe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Company\": companyNames, \"Company Description\": companyDescription, \"Date Victimised\": dateVictimised, \"Number of Clicks\": noClicks})\n",
    "df['Date Victimised'] = pd.to_datetime(df['Date Victimised'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "date_mask = df['Date Victimised'].dt.year < 2023\n",
    "\n",
    "df = df[~date_mask]\n",
    "\n",
    "df.reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9554c0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Medusa_Data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4172687c",
   "metadata": {},
   "source": [
    "## 3) Scraping Glassdoor for more company information\n",
    "\n",
    "More information we need:  \n",
    "- Industry  \n",
    "- Geography  \n",
    "  - Country  \n",
    "\n",
    "Additional information we can scrape (the more we have, the more insights we can infer):\n",
    "- Company Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96a288f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install webdriver-manager\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "285185d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05003b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def location_to_country(location):\n",
    "    base_url = \"https://nominatim.openstreetmap.org/search\"\n",
    "    \n",
    "    params = {\n",
    "        \"addressdetails\": 1,\n",
    "        \"q\": location,\n",
    "        \"format\": \"jsonv2\",\n",
    "        \"limit\": 1\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    if data:\n",
    "        country = data[0].get(\"address\", {}).get(\"country\", \"\")\n",
    "        return country\n",
    "\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84525b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using LinkedIn\n",
    "\n",
    "companyWebsite_list = []\n",
    "industry_list = []\n",
    "headquarter_list = []\n",
    "country_list = []\n",
    "\n",
    "wait = WebDriverWait(driver, 5)\n",
    "\n",
    "for company in companyNames:\n",
    "    try:\n",
    "        driver.get('https://www.google.com')\n",
    "        search = driver.find_element(By.NAME, 'q')\n",
    "        search.send_keys(company + ' linkedin')\n",
    "        search.send_keys(Keys.RETURN)\n",
    "        linkedin_page = driver.find_element_by_tag_name('h3') # clicking the first search result\n",
    "        time.sleep(2) # to limit rate of requests to linkedin\n",
    "        linkedin_page.click()\n",
    "\n",
    "        if \"linkedin\" in driver.current_url:\n",
    "\n",
    "            loaded = wait.until(EC.presence_of_element_located((By.ID, \"main-content\")))\n",
    "\n",
    "            # Check if the scraped company name is even in the linkedin page (sometimes the company data is not in linkedin; false positive)\n",
    "            # try:\n",
    "            #     linkedin_companyName_element = driver.find_element_by_xpath(\"//h1[@class='top-card-layout__title font-sans text-lg papabear:text-xl font-bold leading-open text-color-text mb-0']\")\n",
    "            #     linkedin_companyName = linkedin_companyName_element.text\n",
    "            # except:\n",
    "            #     print(company)\n",
    "            #     continue\n",
    "\n",
    "            # if not substring_included(company, linkedin_companyName):\n",
    "            #     print(company)\n",
    "            #     continue\n",
    "\n",
    "            # bypass sign in modal\n",
    "            try:\n",
    "                sign_in_button_element = driver.find_element_by_xpath(\"//button[@class='sign-in-modal__outlet-btn cursor-pointer btn-md btn-primary']\")\n",
    "                sign_in_button_element.click()\n",
    "                dismiss_button_element = driver.find_element_by_xpath(\"//button[@class='modal__dismiss btn-tertiary h-[40px] w-[40px] p-0 rounded-full indent-0 sign-in-modal__dismiss absolute right-0 cursor-pointer m-[20px]']\")\n",
    "                dismiss_button_element.click()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            CLASS = \".//dd[@class='font-sans text-md text-color-text break-words overflow-hidden']\"\n",
    "\n",
    "            try:\n",
    "                website_element = driver.find_element_by_css_selector('[data-test-id=\"about-us__website\"]')\n",
    "                website_text_element = website_element.find_element_by_xpath(CLASS)\n",
    "                website_text = website_text_element.text\n",
    "            except Exception as e:\n",
    "                website_text = np.nan\n",
    "                print(e)\n",
    "            companyWebsite_list.append(website_text)\n",
    "\n",
    "            try:\n",
    "                industry_element = driver.find_element_by_css_selector('[data-test-id=\"about-us__industry\"]')\n",
    "                industry_text_element = industry_element.find_element_by_xpath(CLASS)\n",
    "                industry_text = industry_text_element.text\n",
    "            except Exception as e:\n",
    "                industry_text = np.nan\n",
    "                print(e)\n",
    "            industry_list.append(industry_text)\n",
    "            \n",
    "            try:\n",
    "                headquarters_element = driver.find_element_by_css_selector('[data-test-id=\"about-us__headquarters\"]')\n",
    "                headquarters_text_element = headquarters_element.find_element_by_xpath(CLASS)\n",
    "                headquarters_text = headquarters_text_element.text\n",
    "\n",
    "                country = location_to_country(headquarters_text)\n",
    "            except Exception as e:\n",
    "                headquarters_text = np.nan\n",
    "                country = np.nan\n",
    "                print(e)\n",
    "            headquarter_list.append(headquarters_text)\n",
    "            country_list.append(country)\n",
    "\n",
    "        else:\n",
    "            companyWebsite_list.append(np.nan)\n",
    "            industry_list.append(np.nan)\n",
    "            headquarter_list.append(np.nan)\n",
    "            country_list.append(np.nan)\n",
    "\n",
    "    except Exception as e:\n",
    "        companyWebsite_list.append(np.nan)\n",
    "        industry_list.append(np.nan)\n",
    "        headquarter_list.append(np.nan)\n",
    "        country_list.append(np.nan)\n",
    "        print(e)\n",
    "    number += 1\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dab5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Company Website\"] = companyWebsite_list\n",
    "df[\"Industry\"] = industry_list\n",
    "df[\"Location\"] = headquarter_list\n",
    "df[\"Country\"] = country_list\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4634c205",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Medusa_Linkedin_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b58b52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
